{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy and Value Iteration in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mdptoolbox\n",
    "import mdptoolbox.example\n",
    "from gym import wrappers\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-10-25 02:37:12,039] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "#create gym evironment\n",
    "env = gym.envs.make(\"CartPole-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.spec.max_episode_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.spec.timestep_limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(4,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"A Markov Decision Problem.\n",
    "\n",
    "    Let ``S`` = the number of states, and ``A`` = the number of acions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    transitions : array\n",
    "        Transition probability matrices. These can be defined in a variety of\n",
    "        ways. The simplest is a numpy array that has the shape ``(A, S, S)``,\n",
    "        though there are other possibilities. It can be a tuple or list or\n",
    "        numpy object array of length ``A``, where each element contains a numpy\n",
    "        array or matrix that has the shape ``(S, S)``. This \"list of matrices\"\n",
    "        form is useful when the transition matrices are sparse as\n",
    "        ``scipy.sparse.csr_matrix`` matrices can be used. In summary, each\n",
    "        action's transition matrix must be indexable like ``transitions[a]``\n",
    "        where ``a`` ∈ {0, 1...A-1}, and ``transitions[a]`` returns an ``S`` ×\n",
    "        ``S`` array-like object.\n",
    "    reward : array\n",
    "        Reward matrices or vectors. Like the transition matrices, these can\n",
    "        also be defined in a variety of ways. Again the simplest is a numpy\n",
    "        array that has the shape ``(S, A)``, ``(S,)`` or ``(A, S, S)``. A list\n",
    "        of lists can be used, where each inner list has length ``S`` and the\n",
    "        outer list has length ``A``. A list of numpy arrays is possible where\n",
    "        each inner array can be of the shape ``(S,)``, ``(S, 1)``, ``(1, S)``\n",
    "        or ``(S, S)``. Also ``scipy.sparse.csr_matrix`` can be used instead of\n",
    "        numpy arrays. In addition, the outer list can be replaced by any object\n",
    "        that can be indexed like ``reward[a]`` such as a tuple or numpy object\n",
    "        array of length ``A``.\n",
    "    discount : float\n",
    "        Discount factor. The per time-step discount factor on future rewards.\n",
    "        Valid values are greater than 0 upto and including 1. If the discount\n",
    "        factor is 1, then convergence is cannot be assumed and a warning will\n",
    "        be displayed. Subclasses of ``MDP`` may pass ``None`` in the case where\n",
    "        the algorithm does not use a discount factor.\n",
    "    epsilon : float\n",
    "        Stopping criterion. The maximum change in the value function at each\n",
    "        iteration is compared against ``epsilon``. Once the change falls below\n",
    "        this value, then the value function is considered to have converged to\n",
    "        the optimal value function. Subclasses of ``MDP`` may pass ``None`` in\n",
    "        the case where the algorithm does not use an epsilon-optimal stopping\n",
    "        criterion.\n",
    "    max_iter : int\n",
    "        Maximum number of iterations. The algorithm will be terminated once\n",
    "        this many iterations have elapsed. This must be greater than 0 if\n",
    "        specified. Subclasses of ``MDP`` may pass ``None`` in the case where\n",
    "        the algorithm does not use a maximum number of iterations.\n",
    "    skip_check : bool\n",
    "        By default we run a check on the ``transitions`` and ``rewards``\n",
    "        arguments to make sure they describe a valid MDP. You can set this\n",
    "        argument to True in order to skip this check.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    P : array\n",
    "        Transition probability matrices.\n",
    "    R : array\n",
    "        Reward vectors.\n",
    "    V : tuple\n",
    "        The optimal value function. Each element is a float corresponding to\n",
    "        the expected value of being in that state assuming the optimal policy\n",
    "        is followed.\n",
    "    discount : float\n",
    "        The discount rate on future rewards.\n",
    "    max_iter : int\n",
    "        The maximum number of iterations.\n",
    "    policy : tuple\n",
    "        The optimal policy.\n",
    "    time : float\n",
    "        The time used to converge to the optimal policy.\n",
    "    verbose : boolean\n",
    "        Whether verbose output should be displayed or not.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    run\n",
    "        Implemented in child classes as the main algorithm loop. Raises an\n",
    "        exception if it has not been overridden.\n",
    "    setSilent\n",
    "        Turn the verbosity off\n",
    "    setVerbose\n",
    "        Turn the verbosity on\n",
    "\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.1  0.9  0.   0. ]\n",
      "  [ 0.1  0.   0.9  0. ]\n",
      "  [ 0.1  0.   0.   0.9]\n",
      "  [ 0.1  0.   0.   0.9]]\n",
      "\n",
      " [[ 1.   0.   0.   0. ]\n",
      "  [ 1.   0.   0.   0. ]\n",
      "  [ 1.   0.   0.   0. ]\n",
      "  [ 1.   0.   0.   0. ]]]\n",
      "[[ 0.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 2.  2.]]\n"
     ]
    }
   ],
   "source": [
    "#P = transtion\n",
    "#if agent tries to move left in s1 there is a 10% probability \n",
    "#they will stay in s1 and 90% probability they will move to s2\n",
    "#Agent is in       S1   S2    S3    S4\n",
    "#left = array ([[  0.1   0.9   0.   0. ]\n",
    "#                [ 0.1   0.    0.9  0. ]\n",
    "#                [ 0.1   0.    0.   0.9]\n",
    "#                [ 0.1   0.    0.   0.9]])\n",
    "\n",
    "#Agent is in      S1    S2   S3  S4\n",
    "#right =array ([[ 1.   0.   0.   0. ]\n",
    "#               [ 1.   0.   0.   0. ]\n",
    "#               [ 1.   0.   0.   0. ]\n",
    "#               [ 1.   0.   0.   0. ]])\n",
    "\n",
    "#R = reward\n",
    "#enter S1 from anywhere get a reward of 0 \n",
    "#S2 and S3 reward of 1 and S4 reward of 2\n",
    "#[[ 0.  0.]\n",
    "#[ 0.  1.]\n",
    "#[ 0.  1.]\n",
    "#[ 2.  2.]]\n",
    "\n",
    "\n",
    "P, R = mdptoolbox.example.forest(4,2)\n",
    "print (P)\n",
    "print (R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#policy iteration\n",
    "pi = mdptoolbox.mdp.PolicyIteration(P, R, 0.9)\n",
    "pi.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10.628820000000008, 11.941020000000009, 13.56102000000001, 15.561020000000008)\n",
      "(0, 0, 0, 0)\n",
      "3\n",
      "0.003929853439331055\n"
     ]
    }
   ],
   "source": [
    "#cumulative reward\n",
    "print (pi.V)\n",
    "#optimal policy\n",
    "print (pi.policy)\n",
    "#max iterations\n",
    "print (pi.iter)\n",
    "#convergence time\n",
    "print (pi.time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#value iteration\n",
    "vi = mdptoolbox.mdp.ValueIteration(P, R, 0.9)\n",
    "vi.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3.0151665990000005, 4.327366599, 5.947366599000001, 7.947366599000001)\n",
      "(0, 0, 0, 0)\n",
      "6\n",
      "0.0006978511810302734\n"
     ]
    }
   ],
   "source": [
    "#cumulative reward\n",
    "print (vi.V)\n",
    "#optimal policy\n",
    "print (vi.policy)\n",
    "#max iterations\n",
    "print (vi.iter)\n",
    "#convergence time\n",
    "print (vi.time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(discount_factor):\n",
    "    global P, R\n",
    "    P, R = mdptoolbox.example.forest(is_sparse=True) \n",
    "    global vi\n",
    "    vi = mdptoolbox.mdp.ValueIteration(P,R,discount_factor)\n",
    "    \n",
    "    vi.run()\n",
    "    \n",
    "    global policy \n",
    "    policy = vi.policy\n",
    "    global V \n",
    "    V = vi.V\n",
    "    \n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(n_s, n_obs, discount_factor):\n",
    "    #create transition probability matrix and reward matrix \n",
    "    \n",
    "    global P, R, V\n",
    "    P, R = mdptoolbox.example.rand(n_s,n_obs, V)\n",
    "    \n",
    "    #policy iteration using mdptoolbox\n",
    "    #parameters : transiton probability matrix,\n",
    "    #rewards matrix ,discount factor\n",
    "    global pi\n",
    "    pi = mdptoolbox.mdp.PolicyIteration(P, R, discount_factor)\n",
    "    \n",
    "    #run the policy iteration\n",
    "    pi.run()\n",
    "    \n",
    "    global policy \n",
    "    policy = pi.policy\n",
    "    V = pi.V\n",
    "    print(V)\n",
    "    \n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4.265663364171351, 4.132235342863298, 4.611430101402171, 4.296594026590925)\n"
     ]
    }
   ],
   "source": [
    "rl_models = [\n",
    "    ('Value Iteration', value_iteration(0.9)),\n",
    "    ('Policy Iteration', policy_iteration(4, 2, 0.9))\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Value Iteration',\n",
       "  ((0, 0, 0), (5.051970000000001, 8.291970000000001, 12.291970000000001))),\n",
       " ('Policy Iteration',\n",
       "  ((1, 1, 1, 1),\n",
       "   (4.265663364171351,\n",
       "    4.132235342863298,\n",
       "    4.611430101402171,\n",
       "    4.296594026590925)))]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
